{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d9832a",
   "metadata": {},
   "source": [
    "# Recognition of abnormality from Breast Thermograms Using ConvXGB\n",
    "\n",
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Pre - Processing ( Anisotropic Diffusion Filtering)](#2)\n",
    "- [3 - Load the Dataset](#3)\n",
    "- [4 - Test and Train Data](#4)\n",
    "- [5 - CNN Model](#5)\n",
    "- [6 - Json File ( For Storing Weights)](#6)\n",
    "- [7 - Loading CNN Model](#7)\n",
    "- [8 - XGBoost](#8)\n",
    "- [9 - Evaluation Metric](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc94953",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "190d30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import keras\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import cv2\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50afc37a",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2- Pre - Processing ( Anisotropic Diffusion Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0aa51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anisodiff(img,niter=1,kappa=50,gamma=0.1,step=(1.,1.),option=1,ploton=False):\n",
    "   \n",
    "    # ...you could always diffuse each color channel independently if you\n",
    "    # really want\n",
    "    if img.ndim == 3:\n",
    "        warnings.warn(\"Only grayscale images allowed, converting to 2D matrix\")\n",
    "        img = img.mean(2)\n",
    "\n",
    "    # initialize output array\n",
    "    img = img.astype('float32')\n",
    "    imgout = img.copy()\n",
    "\n",
    "    # initialize some internal variables\n",
    "    deltaS = np.zeros_like(imgout)\n",
    "    deltaE = deltaS.copy()\n",
    "    NS = deltaS.copy()\n",
    "    EW = deltaS.copy()\n",
    "    gS = np.ones_like(imgout)\n",
    "    gE = gS.copy()\n",
    "\n",
    "    # create the plot figure, if requested\n",
    "    if ploton:\n",
    "        import pylab as pl\n",
    "        from time import sleep\n",
    "\n",
    "        fig = pl.figure(figsize=(20,5.5),num=\"Anisotropic diffusion\")\n",
    "        ax1,ax2 = fig.add_subplot(1,2,1),fig.add_subplot(1,2,2)\n",
    "\n",
    "        ax1.imshow(img,interpolation='nearest')\n",
    "        ih = ax2.imshow(imgout,interpolation='nearest',animated=True)\n",
    "        ax1.set_title(\"Original image\")\n",
    "        ax2.set_title(\"Iteration 0\")\n",
    "\n",
    "        fig.canvas.draw()\n",
    "\n",
    "    for ii in range(niter):\n",
    "\n",
    "        # calculate the diffs\n",
    "        deltaS[:-1,: ] = np.diff(imgout,axis=0)\n",
    "        deltaE[: ,:-1] = np.diff(imgout,axis=1)\n",
    "\n",
    "        # conduction gradients (only need to compute one per dim!)\n",
    "        if option == 1:\n",
    "            gS = np.exp(-(deltaS/kappa)**2.)/step[0]\n",
    "            gE = np.exp(-(deltaE/kappa)**2.)/step[1]\n",
    "        elif option == 2:\n",
    "            gS = 1./(1.+(deltaS/kappa)**2.)/step[0]\n",
    "            gE = 1./(1.+(deltaE/kappa)**2.)/step[1]\n",
    "\n",
    "        # update matrices\n",
    "        E = gE*deltaE\n",
    "        S = gS*deltaS\n",
    "\n",
    "        # subtract a copy that has been shifted 'North/West' by one\n",
    "        # pixel. don't as questions. just do it. trust me.\n",
    "        NS[:] = S\n",
    "        EW[:] = E\n",
    "        NS[1:,:] -= S[:-1,:]\n",
    "        EW[:,1:] -= E[:,:-1]\n",
    "\n",
    "        # update the image\n",
    "        imgout += gamma*(NS+EW)\n",
    "\n",
    "        if ploton:\n",
    "            iterstring = \"Iteration %i\" %(ii+1)\n",
    "            ih.set_data(imgout)\n",
    "            ax2.set_title(iterstring)\n",
    "            fig.canvas.draw()\n",
    "            # sleep(0.01)\n",
    "\n",
    "    return imgout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e2f9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d061df2a",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3 - Load the Dataset\n",
    "\n",
    "The Data Set Consists of total 102 Breast Thermal Images From visual DMI - IR Data Set(https://visual.ic.uff.br/dmi/).\n",
    "- 70 Images (Without Cancer)\n",
    "- 32 Images (With Cancer)\n",
    "- Data Set Zip Folder (https://drive.google.com/file/d/1ILEfL8uose4R7BQGQGZevCw7SCiel62o/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba755b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = \"C:/Users/rsvmu/Downloads/Data\"\n",
    "CATEGORIES = [\"Withcancer\",\"Withoutcancer\"]\n",
    "IMG_SIZE_X = 480\n",
    "IMG_SIZE_Y = 640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcb387",
   "metadata": {},
   "source": [
    "**Note:** Here, The createTrainingData Fucntion besides loading/creating the data for model the two main steps are taking place before giving it to the model:\n",
    " - First step - Is converting the RGB image **(480, 640, 3)** to Gray_scale **(480, 640)**\n",
    " - Second step - Using the above function **anisdiff** filter and apply that on Gray_scale Images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9242fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "def createTrainingData():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(path_test, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            img_array = cv2.imread(os.path.join(path,img),0) #Changing RGB to Gray_Scale \n",
    "            aniso = anisodiff(img_array) # Applying anistropic Diffusion Filter to Gray_scale Images\n",
    "            new_array = cv2.resize(aniso, (IMG_SIZE_X, IMG_SIZE_Y))\n",
    "            training.append([new_array, class_num])\n",
    "createTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "febefa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "y =[]\n",
    "for features, label in training:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, IMG_SIZE_X, IMG_SIZE_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3e1719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "(102, 2)\n"
     ]
    }
   ],
   "source": [
    "X = X.astype('float32')\n",
    "X /= 255\n",
    "from keras.utils import np_utils\n",
    "Y = np_utils.to_categorical(y, 2)\n",
    "print(Y[5])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3096ec",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "# 4 - Test and Train Data\n",
    "- Test Data - 20 Percent of Whole data 102 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc275e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a99c0a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2c65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "nb_classes = 2\n",
    "nb_epochs = 5\n",
    "img_rows, img_columns = 480, 640\n",
    "img_channel = 3\n",
    "nb_filters = 32\n",
    "nb_pool = 2\n",
    "nb_conv = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7f59e",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "# 5 - CNN Model\n",
    "- I have first Used two Convolutional Layers with no padding and for hidden layers took RELU Activation Function and for Output layer we took sigmoid Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa900996",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n",
    "                           input_shape=(480, 640,1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(2,  activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2461eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fe8aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "38/38 [==============================] - 21s 449ms/step - loss: 0.7315 - accuracy: 0.3684 - val_loss: 0.6931 - val_accuracy: 0.6154\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 16s 433ms/step - loss: 0.6914 - accuracy: 0.7105 - val_loss: 0.6572 - val_accuracy: 0.6154\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 17s 438ms/step - loss: 0.7010 - accuracy: 0.6184 - val_loss: 0.6931 - val_accuracy: 0.3846\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 17s 450ms/step - loss: 0.6931 - accuracy: 0.2895 - val_loss: 0.6931 - val_accuracy: 0.3846\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 17s 448ms/step - loss: 0.6931 - accuracy: 0.2895 - val_loss: 0.6931 - val_accuracy: 0.3846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23499ef83a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size = batch_size, epochs = nb_epochs, verbose = 1, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07846b7b",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "# 6 - Json File ( For Storing Weights)\n",
    "- After Completeing the CNN we are storing the weights into json file and read the file to give this weights to a XgBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e084b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_model = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8973b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(json_model)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca7e26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train,axis=(0,1,2))\n",
    "std = np.std(X_train,axis=(0,1,2))\n",
    "X_train = (X_train-mean)/(std+1e-7)\n",
    "X_test = (X_test-mean)/(std+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff098209",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "# 7 - Loading CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e144fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_model(X_test, y_test):\n",
    "\t# load json and create model\n",
    "\tjson_file = open('model.json', 'r')\n",
    "\tloaded_model_json = json_file.read()\n",
    "\tjson_file.close()\n",
    "\tloaded_model = model_from_json(loaded_model_json)\n",
    "\t# load weights into new model\n",
    "\tloaded_model.load_weights(\"model.h5\")\n",
    "\t \n",
    "\t# evaluate loaded model on test data\n",
    "\topt_rms = optimizers.Adam(learning_rate=0.001,decay=1e-6)\n",
    "\tloaded_model.compile(\n",
    "\t\tloss='categorical_crossentropy',\n",
    "\t\toptimizer=opt_rms,\n",
    "\t\tmetrics=['accuracy'])\n",
    "\t'''\n",
    "\ty_test_ = np_utils.to_categorical(y_test, 10)\n",
    "\tscores = loaded_model.evaluate(X_test, y_test_, batch_size=128, verbose=1)\n",
    "\tprint('\\nTest result: %.3f loss: %.3f\\n' % (scores[1]*100,scores[0]))\n",
    "\t'''\n",
    "\treturn loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dabd46b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN model from disk\n"
     ]
    }
   ],
   "source": [
    "cnn_model = load_cnn_model(X_test, y_test)\n",
    "print(\"Loaded CNN model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11eb1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_layer(model, data):\n",
    "\t\n",
    "\ttotal_layers = len(model.layers)\n",
    "\tfl_index = total_layers - 2\n",
    "\tfeature_layer_model = keras.Model(\n",
    "\t\tinputs=model.input,\n",
    "\t\toutputs=model.get_layer(index=fl_index).output)\n",
    "\t\n",
    "\tfeature_layer_output = feature_layer_model.predict(data)\n",
    "\t\n",
    "\treturn feature_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5d402fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted of training data\n",
      "Features extracted of test data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_cnn =  get_feature_layer(cnn_model, X_train)\n",
    "print(\"Features extracted of training data\")\n",
    "X_test_cnn = get_feature_layer(cnn_model, X_test)\n",
    "print(\"Features extracted of test data\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57296e3",
   "metadata": {},
   "source": [
    "<a name='8'></a>\n",
    "# 8 - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1eccaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(X_train, y_train, X_test, y_test):\n",
    "\n",
    "\tdtrain = xgb.DMatrix(\n",
    "\t\tX_train,\n",
    "\t\tlabel=y_train\n",
    "\t)\n",
    "\n",
    "\tdtest = xgb.DMatrix(\n",
    "\t\tX_test,\n",
    "\t\tlabel=y_test\n",
    "\t)\n",
    "\n",
    "\tresults = {}\n",
    "\n",
    "\tparams = {\n",
    "\t\t'max_depth':12,\n",
    "\t\t'eta':0.05,\n",
    "\t\t'objective':'multi:softprob',\n",
    "\t\t'num_class':2,\n",
    "\t\t'eval_metric':'merror'\n",
    "\t}\n",
    "\n",
    "\twatchlist = [(dtrain, 'train'),(dtest, 'eval')]\n",
    "\tn_round = 200\n",
    "\n",
    "\tmodel = xgb.train(\n",
    "\t\tparams,\n",
    "\t\tdtrain,\n",
    "\t\tn_round,\n",
    "\t\twatchlist,\n",
    "\t\tevals_result=results)\n",
    "\n",
    "\tpickle.dump(model, open(\"cnn_xgboost_final.pickle.dat\", \"wb\"))\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038eae44",
   "metadata": {},
   "source": [
    "<a name='9'></a>\n",
    "# 9 - Evalution Metric\n",
    "\n",
    "- merror - Multiclass classification error rate. It is calculated as **#(wrong cases)/#(all cases)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cb2d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and save of CNN-XGBoost Model.\n",
      "[0]\ttrain-merror:0.05263\teval-merror:0.19231\n",
      "[1]\ttrain-merror:0.03947\teval-merror:0.15385\n",
      "[2]\ttrain-merror:0.03947\teval-merror:0.15385\n",
      "[3]\ttrain-merror:0.03947\teval-merror:0.15385\n",
      "[4]\ttrain-merror:0.03947\teval-merror:0.19231\n",
      "[5]\ttrain-merror:0.03947\teval-merror:0.19231\n",
      "[6]\ttrain-merror:0.03947\teval-merror:0.19231\n",
      "[7]\ttrain-merror:0.03947\teval-merror:0.19231\n",
      "[8]\ttrain-merror:0.03947\teval-merror:0.19231\n",
      "[9]\ttrain-merror:0.02632\teval-merror:0.19231\n",
      "[10]\ttrain-merror:0.02632\teval-merror:0.19231\n",
      "[11]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[12]\ttrain-merror:0.02632\teval-merror:0.11538\n",
      "[13]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[14]\ttrain-merror:0.02632\teval-merror:0.11538\n",
      "[15]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[16]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[17]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[18]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[19]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[20]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[21]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[22]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[23]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[24]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[25]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[26]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[27]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[28]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[29]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[30]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[31]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[32]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[33]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[34]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[35]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[36]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[37]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[38]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[39]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[40]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[41]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[42]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[43]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[44]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[45]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[46]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[47]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[48]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[49]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[50]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[51]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[52]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[53]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[54]\ttrain-merror:0.02632\teval-merror:0.15385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rsvmu\\anaconda3\\lib\\site-packages\\xgboost\\core.py:525: FutureWarning: Pass `evals` as keyword args.  Passing these as positional arguments will be considered as error in future releases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[56]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[57]\ttrain-merror:0.02632\teval-merror:0.15385\n",
      "[58]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[59]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[60]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[61]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[62]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[63]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[64]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[65]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[66]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[67]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[68]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[69]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[70]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[71]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[72]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[73]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[74]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[75]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[76]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[77]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[78]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[79]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[80]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[81]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[82]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[83]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[84]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[85]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[86]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[87]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[88]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[89]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[90]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[91]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[92]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[93]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[94]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[95]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[96]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[97]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[98]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[99]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[100]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[101]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[102]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[103]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[104]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[105]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[106]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[107]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[108]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[109]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[110]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[111]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[112]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[113]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[114]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[115]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[116]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[117]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[118]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[119]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[120]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[121]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[122]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[123]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[124]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[125]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[126]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[127]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[128]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[129]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[130]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[131]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[132]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[133]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[134]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[135]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[136]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[137]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[138]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[139]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[140]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[141]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[142]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[143]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[144]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[145]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[146]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[147]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[148]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[149]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[150]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[151]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[152]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[153]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[154]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[155]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[156]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[157]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[158]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[159]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[160]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[161]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[162]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[163]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[164]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[165]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[166]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[167]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[168]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[169]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[170]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[171]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[172]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[173]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[174]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[175]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[176]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[177]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[178]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[179]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[180]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[181]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[182]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[183]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[184]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[185]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[186]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[187]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[188]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[189]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[190]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[191]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[192]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[193]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[194]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[195]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[196]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[197]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[198]\ttrain-merror:0.01316\teval-merror:0.15385\n",
      "[199]\ttrain-merror:0.01316\teval-merror:0.15385\n"
     ]
    }
   ],
   "source": [
    "print(\"Build and save of CNN-XGBoost Model.\")\n",
    "model = xgb_model(X_train_cnn, y_train, X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68b99545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a4324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
